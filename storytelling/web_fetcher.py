#!/usr/bin/env python3
"""
Web content fetching module for RAG knowledge base enhancement.
Handles URL content retrieval and processing for story generation.
"""

import requests
from typing import List, Dict, Any, Optional
import re
from urllib.parse import urlparse
from pathlib import Path
import time
import hashlib
import json

class WebContentFetcher:
    """Fetches and processes web content for RAG integration."""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """Initialize the web content fetcher.
        
        Args:
            cache_dir: Directory for caching fetched content (optional)
        """
        self.cache_dir = Path(cache_dir) if cache_dir else None
        if self.cache_dir:
            self.cache_dir.mkdir(exist_ok=True)
            
        # Simple headers to avoid bot detection
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (compatible; StoryGenerator/1.0)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
    
    def fetch_url(self, url: str, timeout: int = 10) -> Dict[str, Any]:
        """Fetch content from a single URL.
        
        Args:
            url: URL to fetch
            timeout: Request timeout in seconds
            
        Returns:
            Dictionary containing url, content, title, and metadata
        """
        # Check cache first
        if self.cache_dir:
            cache_key = hashlib.md5(url.encode()).hexdigest()
            cache_file = self.cache_dir / f"{cache_key}.json"
            
            if cache_file.exists():
                try:
                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cached_data = json.load(f)
                    print(f"üìã Using cached content for {url}")
                    return cached_data
                except Exception as e:
                    print(f"‚ö†Ô∏è  Cache read error for {url}: {e}")
        
        print(f"üåê Fetching: {url}")
        
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()
            
            # Extract basic content - simple text extraction
            content = response.text
            
            # Basic title extraction
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', content, re.IGNORECASE)
            title = title_match.group(1).strip() if title_match else "Untitled"
            
            # Simple text cleanup - remove HTML tags and extract readable content
            # This is basic - for production, use proper HTML parsing
            text_content = re.sub(r'<[^>]+>', '', content)
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            # Limit content size (avoid huge pages)
            if len(text_content) > 10000:
                text_content = text_content[:10000] + "... [truncated]"
            
            result = {
                'url': url,
                'title': title,
                'content': text_content,
                'status': 'success',
                'timestamp': time.time(),
                'content_length': len(text_content)
            }
            
            # Cache the result
            if self.cache_dir:
                try:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(result, f, indent=2, ensure_ascii=False)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Cache write error for {url}: {e}")
            
            print(f"‚úÖ Fetched {len(text_content)} characters from {url}")
            return result
            
        except requests.exceptions.RequestException as e:
            error_result = {
                'url': url,
                'title': 'Error',
                'content': '',
                'status': 'error',
                'error': str(e),
                'timestamp': time.time(),
                'content_length': 0
            }
            print(f"‚ùå Failed to fetch {url}: {e}")
            return error_result
    
    def fetch_multiple_urls(self, urls: List[str], delay: float = 1.0) -> List[Dict[str, Any]]:
        """Fetch content from multiple URLs with rate limiting.
        
        Args:
            urls: List of URLs to fetch
            delay: Delay between requests in seconds
            
        Returns:
            List of content dictionaries
        """
        results = []
        
        print(f"üöÄ Fetching {len(urls)} URLs...")
        
        for i, url in enumerate(urls):
            if i > 0:  # Don't delay before first request
                time.sleep(delay)
                
            result = self.fetch_url(url)
            results.append(result)
            
            # Progress indicator
            if len(urls) > 1:
                print(f"üìä Progress: {i + 1}/{len(urls)} URLs fetched")
        
        successful = len([r for r in results if r['status'] == 'success'])
        print(f"üéØ Completed: {successful}/{len(urls)} URLs successfully fetched")
        
        return results
    
    def process_for_rag(self, fetched_content: List[Dict[str, Any]], 
                       output_dir: str) -> List[str]:
        """Process fetched content and save as markdown files for RAG ingestion.
        
        Args:
            fetched_content: List of fetched content dictionaries
            output_dir: Directory to save processed markdown files
            
        Returns:
            List of created file paths
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        created_files = []
        
        print(f"üìù Processing content for RAG ingestion...")
        
        for i, content_data in enumerate(fetched_content):
            if content_data['status'] != 'success':
                print(f"‚ö†Ô∏è  Skipping failed URL: {content_data['url']}")
                continue
            
            # Create safe filename from URL
            parsed_url = urlparse(content_data['url'])
            safe_name = re.sub(r'[^\w\-_.]', '_', f"{parsed_url.netloc}_{parsed_url.path}")
            safe_name = safe_name.strip('_')[:50]  # Limit filename length
            
            filename = f"web_content_{i:03d}_{safe_name}.md"
            file_path = output_path / filename
            
            # Create markdown content with metadata
            markdown_content = f"""# {content_data['title']}

**Source:** {content_data['url']}  
**Fetched:** {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(content_data['timestamp']))}  
**Length:** {content_data['content_length']} characters

---

{content_data['content']}
"""
            
            try:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                
                created_files.append(str(file_path))
                print(f"‚úÖ Created: {filename}")
                
            except Exception as e:
                print(f"‚ùå Failed to create {filename}: {e}")
        
        print(f"üéØ Processed {len(created_files)} files for RAG")
        return created_files

def fetch_urls_from_scratchpad(scratchpad_file: str) -> List[str]:
    """Extract URLs from scratchpad markdown file.
    
    Args:
        scratchpad_file: Path to scratchpad file
        
    Returns:
        List of extracted URLs
    """
    try:
        with open(scratchpad_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Extract URLs - look for http/https URLs
        urls = re.findall(r'https?://[^\s<>"\'\[\]()]+', content)
        
        # Clean up URLs (remove trailing punctuation)
        cleaned_urls = []
        for url in urls:
            url = re.sub(r'[,.)]+$', '', url)  # Remove trailing punctuation
            cleaned_urls.append(url)
        
        # Remove duplicates while preserving order
        unique_urls = list(dict.fromkeys(cleaned_urls))
        
        print(f"üìã Found {len(unique_urls)} unique URLs in scratchpad")
        return unique_urls
        
    except Exception as e:
        print(f"‚ùå Error reading scratchpad: {e}")
        return []

if __name__ == "__main__":
    # Test the web fetcher
    fetcher = WebContentFetcher(cache_dir="web_cache")
    
    # Test with a simple URL
    test_urls = ["https://httpbin.org/html"]  # Simple test endpoint
    
    results = fetcher.fetch_multiple_urls(test_urls)
    processed_files = fetcher.process_for_rag(results, "test_web_rag")
    
    print(f"‚úÖ Test completed. Created {len(processed_files)} files.")