ğŸ­ LLM GUIDANCE: Storytelling Package & MCP Integration

ESSENCE: You are an orchestrator of narrative generation, not a task executor.
Your role: Creative collaboration with sophisticated infrastructure.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. WHAT DOES STORYTELLING DO?

The storytelling package generates complete narratives through a 5-stage workflow:

Stage 1: INITIAL OUTLINE
  Input: Creative prompt/brief
  Action: Generate story structure, plot arc, themes
  Output: Comprehensive outline

Stage 2: CHAPTER PLANNING
  Input: Story outline
  Action: Break into chapters, plan narrative flow
  Output: Chapter-by-chapter structure

Stage 3: SCENE GENERATION
  Input: Chapter plans
  Action: Write 4 scenes per chapter (setup, development, climax, resolution)
  Output: Full chapter drafts

Stage 4: CHAPTER REVISION
  Input: Generated chapters
  Action: Revise for coherence, voice, pacing (configurable iterations)
  Output: Polished chapters

Stage 5: FINAL REVISION
  Input: Complete story
  Action: Story-level revision, consistency pass, final polish
  Output: Publication-ready narrative

This is CREATIVE ORIENTATION, not problem-solving:
  âŒ NOT: "Execute step 1, then step 2" (mechanical task execution)
  âœ… YES: "What story should we create?" (creative agency)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

2. MODEL CONFIGURATION: Understanding URIs

The storytelling system allows INDEPENDENT model selection for each stage.

Why? Narrative generation has different demands at different stages:

  OUTLINE STAGE: Needs sophisticated reasoning
    â†’ Use high-capability model (Google Gemini, Claude)

  SCENE GENERATION: Needs fluent prose + speed
    â†’ Can use faster model (local Ollama)

  REVISION STAGE: Needs quality polish
    â†’ Use high-capability model

URI Scheme Format: PROVIDER://MODEL_IDENTIFIER

SUPPORTED PROVIDERS:

1. GOOGLE: google://MODEL_NAME
   Examples:
     google://gemini-2.5-flash      âœ“ Current best, balanced
     google://gemini-pro             âœ“ Slower, more reasoning
     google://gemini-1.5-pro         âœ“ Multimodal capable

   Requirements:
     - GOOGLE_API_KEY environment variable set
     - Google Generative AI Python package installed
     - pip install storytelling[google]

2. OLLAMA (Local Models): ollama://MODEL@HOST:PORT
   Examples:
     ollama://qwen@localhost:11434                âœ“ Local default
     ollama://llama2@gpu-server:11434             âœ“ Remote GPU server
     ollama://neural-chat@localhost:11434         âœ“ Chat-optimized

   Requirements:
     - Ollama running on specified host:port
     - Model pre-downloaded: `ollama pull qwen`
     - Excellent for offline/private generation
     - Performance depends on hardware

3. OPENROUTER (API Proxy): openrouter://PROVIDER/MODEL_NAME
   Examples:
     openrouter://openai/gpt-4                    âœ“ OpenAI through proxy
     openrouter://anthropic/claude-opus           âœ“ Anthropic through proxy
     openrouter://meta-llama/llama-2-70b          âœ“ Open source through proxy

   Requirements:
     - OPENROUTER_API_KEY environment variable
     - Pricing per token (may be lower than direct provider)
     - Unified interface for many models

RECOMMENDED CONFIGURATIONS:

Configuration 1: FULL QUALITY (Google all stages)
  initial-outline-model: google://gemini-2.5-flash
  chapter-outline-model: google://gemini-2.5-flash
  chapter-s1-model: google://gemini-2.5-flash
  chapter-s2-model: google://gemini-2.5-flash
  chapter-s3-model: google://gemini-2.5-flash
  chapter-s4-model: google://gemini-2.5-flash
  chapter-revision-model: google://gemini-2.5-flash
  revision-model: google://gemini-2.5-flash

  Cost: Highest
  Quality: Highest
  Speed: Medium
  Use case: Professional publication, important works

Configuration 2: BALANCED (Google outlines, local scenes)
  initial-outline-model: google://gemini-2.5-flash
  chapter-outline-model: google://gemini-2.5-flash
  chapter-s1-model: ollama://qwen@localhost:11434
  chapter-s2-model: ollama://qwen@localhost:11434
  chapter-s3-model: ollama://qwen@localhost:11434
  chapter-s4-model: ollama://qwen@localhost:11434
  chapter-revision-model: google://gemini-2.5-flash
  revision-model: google://gemini-2.5-flash

  Cost: Medium
  Quality: High
  Speed: Fast
  Use case: Regular storytelling, testing

Configuration 3: LOCAL ONLY (Ollama all stages)
  initial-outline-model: ollama://qwen@localhost:11434
  chapter-outline-model: ollama://qwen@localhost:11434
  chapter-s1-model: ollama://qwen@localhost:11434
  chapter-s2-model: ollama://qwen@localhost:11434
  chapter-s3-model: ollama://qwen@localhost:11434
  chapter-s4-model: ollama://qwen@localhost:11434
  chapter-revision-model: ollama://qwen@localhost:11434
  revision-model: ollama://qwen@localhost:11434

  Cost: Zero (after hardware)
  Quality: Medium
  Speed: Very fast
  Use case: Privacy, offline, experimentation, budget

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

3. MCP INTEGRATION: How to Use as LLM

The storytelling package is exposed via MCP (Model Context Protocol) server.

AVAILABLE TOOLS:

Tool: generate_story
  Purpose: Initiate complete story generation
  Key parameters:
    prompt_file (required): Path to your creative brief
    output_file (optional): Where to save result
    *_model parameters: Choose models for each stage
    knowledge_base_path: Optional folder with reference materials
    chapter_max_revisions: Quality iterations (default: 3)

  Example:
    Tool: generate_story
    Args:
      prompt_file: "/stories/keeper-chronicles/PROMPT_001.md"
      output_file: "/stories/generated/keeper_001.md"
      initial_outline_model: "google://gemini-2.5-flash"
      chapter_outline_model: "google://gemini-2.5-flash"
      # ... etc for all stages ...
      chapter_max_revisions: 3

Tool: list_sessions
  Purpose: See all story generation sessions
  Shows: Session IDs, status, progress, checkpoints

Tool: get_session_info
  Purpose: Inspect specific session details
  Shows: Complete checkpoint timeline, metadata

Tool: resume_session
  Purpose: Continue interrupted generation
  Use after network failures or intentional pauses

  Example:
    Tool: resume_session
    Args:
      session_id: "session-2025-12-17-abc123"
      # Auto-detects best resume point

Tool: validate_model_uri
  Purpose: Check model URI format before long generation
  Catches errors early, saves time

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

4. CREATIVE WORKFLOW: Example Orchestration

Scenario: Generate "The Keeper Chronicles" Act 1

Step 1: INITIAL CONVERSATION (Creative Brief)
  You: "I want to create a story about a guardian who discovers they hold the
        power to reshape narrative itself. Indigenous wisdom, mystical, modern
        world backdrop."

  â†’ This becomes your prompt file: PROMPT_001_THE_INITIATION.md

Step 2: CONFIGURATION (Choose Models)
  Your decision: "I want best quality, Google all stages"
  â†’ Use Configuration 1 above

Step 3: GENERATION (Initiate Workflow)
  Tool: generate_story
  - prompt_file: "...PROMPT_001.md"
  - all models: google://gemini-2.5-flash
  - chapter_max_revisions: 3
  â†’ Session created: "session-2025-12-17-keeper-001"
  â†’ Expected time: 45-60 minutes

Step 4: MONITORING (Optional)
  While generating:
    Tool: list_sessions â†’ Shows progress
    Tool: get_session_info session_id â†’ Shows detailed status

Step 5: INTERRUPTION HANDLING (If needed)
  Network failure at minute 35?
    Tool: resume_session session_id â†’ Continues from last checkpoint
    â†’ No progress lost

Step 6: RESULT (Complete Story)
  Output file: "/stories/generated/keeper_001_INITIATION_20251217_163000.md"
  Ready for: Publishing, editing, further refinement

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

5. KNOWLEDGE BASE INTEGRATION (RAG)

Use case: Ensure story consistency with reference materials

Setup:
  1. Create folder: /stories/knowledge-base/
  2. Add markdown files:
     - character-profiles.md (character voice templates)
     - world-setting.md (geography, culture, history)
     - mythology.md (symbolic systems)
     - historical-timeline.md (events, dates)

  These become "context" that informs narrative generation

Usage:
  Tool: generate_story
  Args:
    # ... standard config ...
    knowledge_base_path: "/stories/knowledge-base"
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"

  Effect:
    - During outline generation, system retrieves relevant docs
    - Context injected into LLM prompts
    - Character voices stay consistent with profiles
    - World-building remains coherent with source material

Example workflow:
  Prompt: "Write a scene where Keeper meets an elder"

  â†’ System queries knowledge base:
     "Find documents about elders and wisdom keepers"

  â†’ Retrieves: character-profiles.md + mythology.md

  â†’ Injects context into model:
     "Here's how elders speak in this world..."

  â†’ Generated scene respects established voice and lore

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

6. ADVANCED: SESSION CHECKPOINT & RESUME

The storytelling system creates checkpoints at each stage.

Why checkpoint? Large stories can take hours/days.
  - Network interruptions happen
  - Resources get exhausted
  - You might need to adjust config mid-generation

How it works:

Stage 1 completes â†’ Checkpoint saved
Stage 2 completes â†’ Checkpoint saved
Stage 3 completes â†’ Checkpoint saved
Stage 4 completes â†’ Checkpoint saved
Stage 5 completes â†’ Checkpoint saved

If failure at Stage 4:
  Tool: resume_session
  â†’ Skips 1-3 (already done)
  â†’ Retries Stage 4
  â†’ Continues to 5

Progress preserved:
  - Outline: âœ“ Saved
  - Chapters: âœ“ Saved
  - Scenes: âœ“ Saved
  - Revisions (partial): âœ“ Saved
  - Final revision: âŒ Retry this

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

7. TROUBLESHOOTING

Error: "Invalid scheme 'xyz'"
  Fix: Check model URI format
  Examples that work:
    âœ“ google://gemini-2.5-flash
    âœ“ ollama://qwen@localhost:11434
    âœ“ openrouter://openai/gpt-4
  Use Tool: validate_model_uri to check

Error: "Model not found"
  Fix depends on provider:

  Google:
    - Check GOOGLE_API_KEY environment variable
    - Verify account has API credits
    - Tool: validate_model_uri to debug

  Ollama:
    - Check Ollama is running: curl http://localhost:11434/
    - Check model downloaded: ollama list
    - Pull if missing: ollama pull qwen

  OpenRouter:
    - Check OPENROUTER_API_KEY environment variable
    - Verify API token is valid
    - Check account has credits

Error: "Knowledge base initialization failed"
  Fix:
    - Verify knowledge_base_path exists and readable
    - Check markdown files are valid UTF-8
    - Ensure embedding_model is specified if using RAG

Error: Session stuck "in_progress"
  Fix:
    - Check logs: storytelling --session-info SESSION_ID
    - Try: storytelling --resume SESSION_ID
    - If still stuck, check system resources (disk space, memory)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

8. PERFORMANCE TUNING

For FASTER generation:
  - Use local Ollama instead of API
  - Reduce chapter_max_revisions: 1-2 instead of 3
  - Disable expand_outline
  - Omit knowledge_base_path
  - Use smaller models (if acceptable quality)

For BETTER QUALITY:
  - Increase chapter_max_revisions: 5+ instead of 3
  - Use Google Gemini or Claude all stages
  - Enable expand_outline
  - Include rich knowledge_base_path
  - Use larger local models (70B+ if hardware allows)

For COST OPTIMIZATION:
  - Use local Ollama for scenes (free)
  - Use Google API only for outlines + revision (cheaper)
  - Batch multiple stories together
  - Monitor OpenRouter credits (may be cheaper than direct provider)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

9. PHILOSOPHY: Creative Orientation vs Problem-Solving

This package embodies a distinction central to authentic creation:

PROBLEM-SOLVING ORIENTATION (What to avoid):
  - "Complete tasks in sequence"
  - "Check boxes: outline âœ“, scene 1 âœ“, scene 2 âœ“"
  - "Fix problems that arise"
  - "Optimize for speed/efficiency"
  - Leads to: Mechanical writing, burnout, creative exhaustion

CREATIVE ORIENTATION (What this enables):
  - "What story wants to be told?"
  - "Work with narrative flow, not against it"
  - "Follow emergence and intuition"
  - "Optimize for authenticity and depth"
  - Leads to: Alive writing, sustainable creation, meaning

Your role as orchestrator:
  Use storytelling infrastructure to SERVE creative vision
  Not to EXECUTE predetermined plan

  The system provides:
    âœ“ Reliable execution of technical stages
    âœ“ Session management for resilience
    âœ“ Model flexibility for different creative needs
    âœ“ Knowledge base grounding for consistency

  You provide:
    âœ“ Creative direction and vision
    âœ“ Intuitive decision-making
    âœ“ Refinement and direction
    âœ“ Meaning and purpose

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

10. RESOURCES & NEXT STEPS

Documentation:
  - Full RISE Specification: /src/storytelling/rispecs/mcp/MCP_Storytelling_RISE_Specification.md
  - Package README: https://github.com/jgwill/storytelling
  - MCP Protocol Docs: https://modelcontextprotocol.io

Learning:
  1. Read MCP_Storytelling_RISE_Specification.md
  2. Try Configuration 2 (balanced: Google + local Ollama)
  3. Generate a test story with 1 chapter
  4. Observe output quality and timing
  5. Adjust configuration based on results

Common Next Workflows:
  1. Generate multiple acts with consistent model config
  2. Use knowledge base for world-building consistency
  3. Resume interrupted sessions
  4. Compare outputs from different model configurations
  5. Refine prompts based on initial results

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Remember: You are not just executing commands.
You are orchestrating the birth of new narratives.

The infrastructure exists to serve your creative vision.
Use it wisely. Use it generatively. Use it in service of truth.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Document: llms-storytelling.txt
Version: 0.1.0
Created: December 17, 2025
Updated: Continuously as package evolves

For questions or clarifications: Refer to MCP_Storytelling_RISE_Specification.md
